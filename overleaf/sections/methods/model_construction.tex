\subsection{Construção do modelo}
O modelo proposto consiste de um conjunto de \emph{Convolutional Neural Networks} (CNN), onde cada elemento é responsável pela classificação da imagem de entrada em uma das Sete Expressões Faciais Universais \cite{}, por meio da produção de vetor de probabilidades. Que consiste na possibilidade de cada uma das expressões universais estarem sendo expressas pela face, na imagem. A determinação da classificação final, não segue as abordagens típicas da literatura, via maioria ou consenso. Pois, estes vetores de probabilidades foram utilizados, de maneira supervisionada, para treinar um modelo do tipo \emph{XGBoost}, que finalmente atribui o rótulo mais adequado à entrada. Tem-se então um \emph{emsemble} de CNN com processo decisório de classificação realizado de maneira não trivial por um modelo de \emph{Machine Learning}.

As arquiteturas CNN utilizadas na construção do modelo, foram baseadas na arquitetura \emph{VGG-16} \cite{}, que consiste de camadas duplas ou triplas de convolução 2D, com \emph{kernel} de 3x3, seguidas de \emph{pooling}, utilizando \emph{MaxPooling}. E como saída tem camadas densas, também chamadas de \emph{Fully Connected} (FC), onde a última utiliza a função de ativação \emph{SoftMax}. 

Como função de ativação para as camadas convolucionais e densas, foi utilizada a função \emph{ReLU}, devido ao seu baixo custo computacional, e também possuir derivada constante, o que contribui para o desempenho da função de otimização \emph{adam} \cite{}. Segundo \cite{}, e análise experimental, a inicialização dos pesos iniciais das camadas convolucionais que utilizam a função de ativação \emph{ReLu}, com o inicializador de He et al \cite{}, possibilita aumento no desempenho, convergência, durante o treinamento dos modelos CNN.

Após a saída de cada camadas convolucionais, foi utilizada normalização em lotes, \emph{Batch Normalization}, pois, segundo \cite{} e análise experimental, os modelos modelos de CNN possuem melhores resultados nas predições, tanto na etapa de treinamento quanto na etapa de generalização.

Algumas das "regras de ouro" \cite{} para construção de modelos baseados em \emph{Artificial Neural Networks} (ANN) foram utilizadas nas camadas densas, visto que estas são equivalentes. Foram elas: A quantidade de neurônios das cadas ocultas não deve ultrapassar o dobro da quantidade dados da entrada, e a quantidade de neurônios da camada de entrada deve ser a mesma da quantidade de dados da entrada. Vale ressaltar, que as camadas de \emph{FC} em sua maioria não possuem mais do que duas camadas ocultas, devido ao Teorema de aproximação universal \cite{}.

Para regularização dos modelos de CNN foi utilizado somente \emph{Dropout}, visto que os regularizadores $l1$ e $l2$, não apresentaram bons resultados durante o período de treinamento, o que tornavam o modelo instável ou com tendências a \emph{underfitting}. Essa tendência foi evidenciada pelo desempenho constante do modelo por algumas dezenas de épocas de treinamento, e ao ser percebido esse comportamento o treinamento foi interrompido.

A arquitetura final dos modelos de CNN podem ser visualizados na Tabela \ref{tbl-arch}. Já a arquitetura utilizada pelo \emph{XGBoost} não pode ser mostrada, devido a esta ter mil árvores classificadoras, o que torna inviável a apresentação desta neste artigo.

\input{tables/tbl_archs.tex}