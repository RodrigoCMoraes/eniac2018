
# coding: utf-8

# # Facial Expression Classification with FER2013
# 
# ## Rodrigo Moraes (rdcmdev@gmail.com)
# 
# <img align="left" width="150" height="150" src="https://avatars1.githubusercontent.com/u/23252082?s=400&u=9a693e90761b50f89d7ae8a85b6f04f14400fd16&v=4">
# 
# <br/><br/><br/><br/><br/><br/><br/><br/>
# 
# ## Required packages
# 
# 1. Pandas
# 2. OpenCv
# 3. Numpy
# 4. Scikit Learning
# 5. Keras
# 6. Matplotlib
# 7. Seaborn
# 
# Use most recent version of this packages

# In[1]:


# import required packages
import pandas as pd

import cv2

import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import fbeta_score

from keras.models import Sequential, load_model
from keras.layers import BatchNormalization, Activation, GlobalAveragePooling2D, UpSampling2D
from keras.layers.core import Flatten, Dense, Dropout
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D
from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from keras import regularizers
from keras.applications.xception import Xception
from keras.applications.vgg16 import VGG16
from keras import backend as K

import matplotlib.pyplot as plt

import seaborn as sns


# In[2]:


# load dataset from csv file

fer2013 = pd.read_csv("../datasets/fer2013.csv")
fer2013.head()


# In[3]:


EMOTION_TRAINED = 'angry'
emotions_labels_map = {'angry':0,
                       'disgust':1, 
                       'fear':2, 
                       'happy':3, 
                       'sad':4, 
                       'surprise':5, 
                       'neutral':6}

# fer2013.emotion = [1 if x == emotions_labels_map[EMOTION_TRAINED] else 0 for x in fer2013['emotion']]
print(fer2013.groupby(['emotion']).agg('count')['pixels'])


# In[4]:


# group dataset by usage

groups = fer2013.groupby('Usage')
groups.count()


# In[5]:


# get group - training

training_data = groups.get_group('Training')

# get groups on training split by emotion

training_emotions = training_data.groupby('emotion')
training_emotions.count()


# In[6]:


# get group - test

test_data = groups.get_group('PublicTest')
# get groups on test split by emotion

test_emotions = test_data.groupby('emotion')
test_emotions.count()


# In[7]:


# get group - validation

validation_data = groups.get_group('PrivateTest')

# get groups on validation split by emotion

validation_emotions = validation_data.groupby('emotion')
validation_emotions.count()


# In[8]:


def prepare_dataset(dataframe, image_size={'width':48, 'height':48}):
    faces_images = []
    pixels = dataframe['pixels'].tolist()
    for pixel_sequence in pixels:
        face = [int(pixel) for pixel in pixel_sequence.split(' ')]
        face = np.asarray(face).reshape(image_size['width'], image_size['height'])
        face = cv2.resize(face.astype('uint8'), (image_size['width'], image_size['height']))
        face = cv2.equalizeHist(face)
#         face = cv2.Laplacian(face, cv2.CV_64F)
        
        faces_images.append(face.astype('float32') / 255.0)
    faces_images = np.asarray(faces_images)
    faces_images = np.expand_dims(faces_images, -1) # (1, 48, 48)
    emotions_labels = pd.get_dummies(dataframe['emotion']).as_matrix()
    return faces_images, emotions_labels

# prepare dataset to training, test and validate model
training_faces, training_emotions = prepare_dataset(training_data)
print('training', len(training_faces), len(training_emotions))
test_faces, test_emotions = prepare_dataset(test_data)
print('test', len(test_faces), len(test_emotions))
validation_faces, validation_emotions = prepare_dataset(validation_data)
print('validation', len(validation_faces), len(validation_emotions))


# In[9]:


# normalize faces

training_faces = training_faces.astype('float32')
test_faces = test_faces.astype('float32')
validation_faces = validation_faces.astype('float32')


# In[10]:


# get size dataset elements
num_samples, num_classes = training_emotions.shape
print(num_samples, num_classes)


# In[11]:


# split and randomize data
x_train_part1, x_train_part2, y_train_part1, y_train_part2 = train_test_split(training_faces, training_emotions, test_size=0.3, random_state=1)
x_train, y_train = np.vstack((x_train_part1, x_train_part2)), np.vstack((y_train_part1, y_train_part2))
print('training', len(x_train), len(y_train))

x_test_part1, x_test_part2, y_test_part1, y_test_part2 = train_test_split(test_faces, test_emotions, test_size=0.3, random_state=1)
x_test, y_test = np.vstack((x_test_part1, x_test_part2)), np.vstack((y_test_part1, y_test_part2))
print('test', len(x_test), len(y_test))

x_validation_part1, x_validation_part2, y_validation_part1, y_validation_part2 = train_test_split(validation_faces, validation_emotions, test_size=0.3, random_state=1)
x_validation, y_validation = np.vstack((x_validation_part1, x_validation_part2)), np.vstack((y_validation_part1, y_validation_part2))
print('validation', len(x_validation), len(y_validation))


# In[12]:


# create data augmention
data_generator = ImageDataGenerator(
                        featurewise_center=False,
                        featurewise_std_normalization=False,
                        rotation_range=10,
                        width_shift_range=0.1,
                        height_shift_range=0.1,
                        zoom_range=.1,
                        horizontal_flip=True)

data_generator.fit(x_train)


# In[13]:


# create training data

MODEL_BASE_PATH = "../trained_models/"
MODEL_BASE_NAME = "cnn_fer2013_"# + EMOTION_TRAINED
MODEL_FILENAME_LOG = MODEL_BASE_PATH + MODEL_BASE_NAME + '.log'
MODEL_FILENAME = MODEL_BASE_PATH + MODEL_BASE_NAME

patience = 10
batch_size = 64
num_epochs = 1000

# callbacks
csv_logger = CSVLogger(MODEL_FILENAME_LOG, append=False)

# EarlyStopping - https://keras.io/callbacks/#earlystopping
early_stop = EarlyStopping(monitor='val_loss', patience=patience, mode='min')

# ReduceLROnPlateau - ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau('val_loss', factor=0.5, patience=(patience//4))

model_names = MODEL_FILENAME + 'fbeta{val_fbeta:.2f}-{val_acc:.2f}.hdf5' #.{epoch:02d}-{val_acc:.2f}

# ModelCheckpoint - https://keras.io/callbacks/#modelcheckpoint
model_checkpoint = ModelCheckpoint(model_names,  monitor='val_fbeta', save_best_only=True, mode='max')

callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]


# In[14]:


def fbeta(y_true, y_pred, threshold_shift=0):
    beta = 1

    # just in case of hipster activation at the final layer
    y_pred = K.clip(y_pred, 0, 1)

    # shifting the prediction threshold from .5 if needed
    y_pred_bin = K.round(y_pred + threshold_shift)

    tp = K.sum(K.round(y_true * y_pred_bin), axis=1) + K.epsilon()
    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)), axis=1)
    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)), axis=1)

    precision = tp / (tp + fp)
    recall = tp / (tp + fn)

    beta_squared = beta ** 2
    return K.mean((beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon()))


# In[15]:


model = Sequential()
model.add(Convolution2D(filters=32, activation='relu', kernel_size=(3, 3), padding='same', input_shape=(48, 48, 1), kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Dropout(.1))
model.add(Convolution2D(filters=32, activation='relu', kernel_size=(3, 3), strides=2, padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Dropout(.5))

model.add(Convolution2D(filters=32, activation='relu', kernel_size=(3, 3), padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Convolution2D(filters=32, activation='relu', kernel_size=(3, 3), strides=2, padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Dropout(.5))

model.add(Convolution2D(filters=32, activation='relu', kernel_size=(3, 3), padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Convolution2D(filters=32, activation='relu', kernel_size=(3, 3), strides=2, padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Dropout(.5))

model.add(Convolution2D(filters=64, activation='relu', kernel_size=(3, 3), padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Convolution2D(filters=64, activation='relu', kernel_size=(3, 3), padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))
model.add(Dropout(.5))

model.add(Convolution2D(filters=64, activation='relu', kernel_size=(3, 3), padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Convolution2D(filters=64, activation='relu', kernel_size=(3, 3), padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))
model.add(Dropout(.5))

model.add(Convolution2D(filters=64, activation='relu', kernel_size=(3, 3), padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Convolution2D(filters=64, activation='relu', kernel_size=(3, 3), padding='same', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))
model.add(Dropout(.5))

model.add(Flatten())
model.add(Dense(64, kernel_initializer='he_normal', activation='relu'))
model.add(Dropout(.3))
model.add(Dense(48, kernel_initializer='he_normal', activation='relu'))
model.add(Dense(48, kernel_initializer='he_normal', activation='relu'))
model.add(Dense(7, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[fbeta, 'accuracy'])
    
model.summary()


# In[16]:


# model_name = 'simple_CNN.985-0.66.hdf5'
# model = load_model("../trained_models/" + model_name, custom_objects={'fbeta': fbeta})


# In[ ]:


## train network
history = model.fit_generator(data_generator.flow(x_train, y_train, batch_size),
                    steps_per_epoch=len(x_train) / batch_size,
                    epochs=num_epochs, verbose=True, callbacks=callbacks,
                    validation_data=(x_test, y_test))


# In[ ]:


# evaluate model with accuracy metrics
score = model.evaluate(x_validation, y_validation)

print('Test score:', score[0])
print('Test accuracy:', score[1])


# In[ ]:


# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.savefig(MODEL_BASE_NAME + '_accuracy.png')
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.savefig(MODEL_BASE_NAME + '_loss.png')


# In[ ]:


# Plot a confusion matrix
emotions_text = ['angry', 'disgust', 'feat', 'happy', 'sad', 'surprise', 'neutral']

y_pred = model.predict_classes(x_validation)
y_true = np.asarray([np.argmax(i) for i in y_validation])

cm = confusion_matrix(y_true, y_pred)
cm_normalised = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.set(font_scale=4.5) 
fig, ax = plt.subplots(figsize=(30,20))
ax = sns.heatmap(cm_normalised, annot=True, linewidths=2.5, square=True, linecolor="Green", 
                    cmap="Greens", yticklabels=emotions_text, xticklabels=emotions_text, vmin=0, vmax=np.max(cm_normalised), 
                    fmt=".2f", annot_kws={"size": 50})
ax.set(xlabel='Predicted label', ylabel='True label')
fig.savefig(MODEL_BASE_NAME + '_accuracy.png')

